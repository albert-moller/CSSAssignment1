{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to github: https://github.com/albert-moller/CSSAssignment1.git\n",
    "\n",
    "Group members: Albert Frisch Møller (s214610) and Mark Andrawes (s214654)\n",
    "\n",
    "For this assignment each group member contributed equally to every aspect of the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Web-Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary Web Scraping and Data Storage packages\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import networkx as nx\n",
    "import json\n",
    "\n",
    "if not os.path.exists(\"./data\"):\n",
    "    os.mkdir(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique researchers is 1856\n"
     ]
    }
   ],
   "source": [
    "LINK = \"https://ic2s2-2023.org/program\"\n",
    "r = requests.get(LINK)\n",
    "soup = BeautifulSoup(r.content)\n",
    "researchers = []\n",
    "\n",
    "table_rows = soup.find_all(\"tr\")\n",
    "for row in table_rows:\n",
    "    for tag in row.find_all(\"a\"):\n",
    "        if re.search(\"Keynote\", tag.text):\n",
    "            string = tag.text\n",
    "            string = string.replace(\"Keynote - \", \"\")\n",
    "            string = string.strip()\n",
    "            researchers.append(string)\n",
    "\n",
    "plenaries = soup.find_all(class_='nav_list')\n",
    "\n",
    "for plenary in plenaries:\n",
    "    italic_names = plenary.find_all('i')\n",
    "    italic_names = [i.get_text() for i in italic_names]\n",
    "\n",
    "    for entry in italic_names:\n",
    "        speakers = entry.split(',')\n",
    "        for speaker in speakers:\n",
    "            name = speaker.strip()\n",
    "            if name not in researchers:\n",
    "                researchers.append(speaker)\n",
    "\n",
    "researchers_df = pd.DataFrame(researchers, columns=['Full Name'])\n",
    "researchers_df.to_csv('data/ics2_researchers.csv', index=False)\n",
    "\n",
    "print(f\"The number of unique researchers is {len(researchers_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many unique researchers do you get?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained that there are 1856 unique researchers present at the International Conference in Computational Social Science for 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain the process you followed to web-scrape the page. Which choices did you make to accurately retreive as many names as possible? Which strategies did you use to assess the quality of your final list? Explain your reasoning and your choices. (answer in max 150 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To webscrape the IC2S2 2023 program page, we inspected the HTML structure to identify the elements containing researcher names. Using BeautifulSoup, we collected the table rows ('tr') to extract the names of the keynote speakers. Additionally, we collected each 'i' tag within each plenary (found in the class 'nav_list') which contained multiple names of participants - we split these names and appended each one to the list. Finally, we stored the data in a Pandas DataFrame and saved as csv.\n",
    "\n",
    "\n",
    "To retrieve the names accurately, we removed the 'Keynote -' prefix to cleanly retrieve the names of the keynote speakers. We used the '.strip()' function to remove any leading/trailing whitespace. This was done to ensure that we can accurately compare names when checking for duplicates. We assessed the quality by checking for duplicates by only adding new names to the list. This ensured the list only included unique participants.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Ready Made vs Custom Made Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What are pros and cons of the custom-made data used in Centola's experiment (the first study presented in the lecture) and the ready-made data used in Nicolaides's study (the second study presented in the lecture)? You can support your arguments based on the content of the lecture and the information you read in Chapter 2.3 of the book (answer in max 150 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Centola's experiment, custom-made data was collected specifically for the study, allowing for full control over the environment and the variables. As explained in “Bit by Bit”, this enhances the relevance and accuracy of the findings to the research question, which is a significant advantage. Additionally, this reduces ethical concerns in the data. On the other hand, custom-made data requires significant resources and time to create, which is a disadvantage.\n",
    "\n",
    "\n",
    "In Nicolaides's study, ready-made data was used. As described in “Bit by Bit”, an advantage of this is the free accessibility of the data and its broader context. However, this type of data usually contains biases and may require adjustments in the methodology to align with the available data. Moreover, there may be issues with the quality and completeness of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How do you think these differences can influence the interpretation of the results in each study? (answer in max 150 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differences between custom-made and ready-made data can significantly influence the interpretation of results in the studies. For Centola's experiment, the control over the experimental conditions allowed for clearer ways of linking effects to specific causes, leading to these insights being more reliable. However, this control also means that the results might be less generalizable to real-world settings, as the controlled environment may not capture all realistic external factors. \n",
    "\n",
    "On the other hand, Nicolaides's use of ready-made data derived from existing datasets offers insights that are more reflective of real-world behaviors. This enhances the generalizability of the findings but makes it challenging to determine links between effects and causes due to potential confounders and biases in the data.\n",
    "Hence, while custom-made data provides cleaner, more controlled insights, ready-made data offers broader perspectives on natural behaviors. These differences mean that we must carefully interpret results based on the data type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3: Gathering Research Articles using the OpenAlex API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Obtaining authors dataset:   0%|          | 0/1856 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Obtaining authors dataset: 100%|██████████| 1856/1856 [22:32<00:00,  1.37it/s]\n"
     ]
    }
   ],
   "source": [
    "#Step 1) Obtain ISC2 Research OpenAlex IDs (using the \"authors\" endpoint)\n",
    "\n",
    "researchers = pd.read_csv(\"data/ics2_researchers.csv\")\n",
    "openalexids_df = pd.DataFrame(columns = ['id', 'display_name', 'works_api_url', 'h_index', 'works_count', 'country_code', 'cited_by_count'])\n",
    "\n",
    "url = \"https://api.openalex.org/authors\"\n",
    "index = 0\n",
    "\n",
    "for researcher in tqdm(researchers[\"Full Name\"], desc=\"Obtaining authors dataset\"):\n",
    "    params = {'search': researcher}\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    if not data['results']:\n",
    "        continue\n",
    "\n",
    "    author_data = data['results']\n",
    "    if len(author_data) > 1:\n",
    "        author_data = max(author_data, key=lambda x: x.get('relevance_score', 0))\n",
    "    else:\n",
    "        author_data = data['results'][0]\n",
    "\n",
    "    try:\n",
    "        id = author_data['id']\n",
    "        display_name = author_data['display_name']\n",
    "        works_api_url = author_data['works_api_url']\n",
    "        h_index = author_data['summary_stats']['h_index']\n",
    "        works_count = author_data['works_count']\n",
    "        country_code = author_data['last_known_institution']['country_code']\n",
    "        cited_by_count = author_data['cited_by_count'] \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    df_index = len(openalexids_df)\n",
    "    openalexids_df.loc[df_index] = [id, display_name, works_api_url, h_index, works_count, country_code, cited_by_count]\n",
    "\n",
    "    if index % 30 == 0:\n",
    "        openalexids_df.to_csv(\"data/ics2_authors.csv\", index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantiative Disciplines Filter: C33923547|C121332964|C41008148\n",
      "Computational Social Science Filter: C144024400|C15744967|C162324750|C17744445\n"
     ]
    }
   ],
   "source": [
    "#Step 2) Use the \"concepts\" endpoint to obtain the concepts IDs for Sociology, Psychology, Economics, Political Science, Mathematics, Physics, Computer Science\n",
    "\n",
    "fields = [\"Sociology\", \"Psychology\", \"Economics\", \"Political Science\", \"Mathematics\", \"Physics\", \"Computer Science\"]\n",
    "concepts_id = {}\n",
    "\n",
    "for field in fields:\n",
    "    url = f\"https://api.openalex.org/concepts?search={field}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    if data['results']:\n",
    "        concept_id = data['results'][0]['id']\n",
    "        _, concept_id = os.path.split(concept_id)\n",
    "        concepts_id[field] = concept_id\n",
    "\n",
    "quantitative_disciplines_filter = f\"{concepts_id['Mathematics']}|{concepts_id['Physics']}|{concepts_id['Computer Science']}\"\n",
    "css_filter = f\"{concepts_id['Sociology']}|{concepts_id['Psychology']}|{concepts_id['Economics']}|{concepts_id['Political Science']}\"\n",
    "\n",
    "print(f\"Quantiative Disciplines Filter: {quantitative_disciplines_filter}\")\n",
    "print(f\"Computational Social Science Filter: {css_filter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1501/1501 [05:16<00:00,  4.75it/s]\n"
     ]
    }
   ],
   "source": [
    "#Step 3) Use the \"works\" endpoint to obtain all the research articles authored by ICS2 participants\n",
    "\n",
    "def fetch_author_works(author_id, social_sciences_ids, quantitative_disciplines_ids):\n",
    "    _, openalex_id = os.path.split(author_id)\n",
    "    filters = (\n",
    "        f'author.id:{openalex_id}|{openalex_id}',\n",
    "        f'cited_by_count:>10',\n",
    "        f'authors_count:<10',\n",
    "        f'concepts.id:({social_sciences_ids})',\n",
    "        f'concepts.id:({quantitative_disciplines_ids})'\n",
    "    )\n",
    "    url = f\"https://api.openalex.org/works?filter={','.join(filters)}per-page=200\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    return data['results']\n",
    "\n",
    "def process_author_works(author_id):\n",
    "    author_works = fetch_author_works(author_id, css_filter, quantitative_disciplines_filter)\n",
    "    papers_data = []\n",
    "    abstracts_data = []\n",
    "    for work in author_works:\n",
    "        work_id = work['id']\n",
    "        publication_year = work['publication_year']\n",
    "        cited_by_count = work['cited_by_count']\n",
    "        title = work['title']\n",
    "        abstract_inverted_index = work.get('abstract_inverted_index', {})\n",
    "        author_ids = [author['author']['id'] for author in work['authorships']]\n",
    "        \n",
    "        papers_data.append({\n",
    "            'id': work_id,\n",
    "            'publication_year': publication_year,\n",
    "            'cited_by_count': cited_by_count,\n",
    "            'author_ids': author_ids,\n",
    "        })\n",
    "        \n",
    "        abstracts_data.append({\n",
    "            'id': work_id,\n",
    "            'title': title,\n",
    "            'abstract_inverted_index': abstract_inverted_index,\n",
    "        })\n",
    "    return papers_data, abstracts_data\n",
    "        \n",
    "    \n",
    "ICS2_authors = pd.read_csv(\"data/ics2_authors.csv\")\n",
    "ICS2_authors_filtered = ICS2_authors[(ICS2_authors['works_count'] >= 5) & (ICS2_authors['works_count'] <= 5000)]\n",
    "author_ids = ICS2_authors_filtered['id'].tolist()\n",
    "\n",
    "all_papers_data = []\n",
    "all_abstracts_data = []\n",
    "\n",
    "results = Parallel(n_jobs=4, backend=\"threading\")(delayed(process_author_works)(author_id) for author_id in tqdm(author_ids))\n",
    "\n",
    "for papers_data, abstracts_data in results:\n",
    "    all_papers_data.extend(papers_data)\n",
    "    all_abstracts_data.extend(abstracts_data)\n",
    "\n",
    "papers_df = pd.DataFrame(all_papers_data, columns=[\"id\", \"publication_year\", \"cited_by_count\", \"author_ids\"])\n",
    "abstracts_df = pd.DataFrame(all_abstracts_data, columns=[\"id\", \"title\", \"abstract_inverted_index\"])\n",
    "papers_df.to_csv('data/papers.csv', index=False)\n",
    "abstracts_df.to_csv('data/abstracts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of works listed in the ICSS2 papers dataframe: 12986\n",
      "Number of unique researchers that have co-authored the found works: 13027\n"
     ]
    }
   ],
   "source": [
    "# Step 4) Determine how many unique researchers have co-authored these works\n",
    "\n",
    "co_authors = papers_df['author_ids']\n",
    "unique_co_authors = []\n",
    "\n",
    "for row in co_authors:\n",
    "    for author in row:\n",
    "\n",
    "        if author not in unique_co_authors:\n",
    "            unique_co_authors.append(author)\n",
    "\n",
    "print(f\"Number of works listed in the ICSS2 papers dataframe: {len(papers_df)}\")\n",
    "print(f\"Number of unique researchers that have co-authored the found works: {len(unique_co_authors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How many works are listed in your IC2S2 papers dataframe? How many unique researchers have co-authored these works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 12972 works listed in the ICS2S2 papers dataframe. Additionally, there are 13015 unique researchers that have co-authored the found works.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe the strategies you implemented to make your code more efficient. How did your approach affect your code's execution time? (answer in max 150 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enhance code efficiency, our approach included batching requests to the OpenAlex API to query multiple authors simultaneously and applying filters directly within API requests to minimize data processing. We used Joblib's Parallel function which enabled concurrent processing, significantly reducing the total execution time. By adjusting the API's per-page limit to 200 works and implementing pagination, the script efficiently handled large datasets. These strategies ensured that the data retrieval process was both fast and resource-efficient, and the executing time was massively reduced by implementing these strategies compared to the case where they were not implemented.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reflect on the rationale behind setting specific thresholds for the total number of works by an author, the citation count, the number of authors per work, and the relevance of works to specific fields. How do these filtering criteria contribute to the relevance of the dataset you compiled? Do you believe any aspects of Computational Social Science research might be underrepresented or overrepresented as a result of these choices? (answer in max 150 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the specified filtering criteria is to enhance the dataset’s relevance and manageability. Setting thresholds for an author’s total work count ensured that we only included those with a significant contribution to the field of Computational Social Science, while the citation count filter ensured that only works with greater impact were included. Limiting works authored by fewer than 10 individuals helped focus on collaborations that are more typical in Computational Social Science, and avoided overly large teams that might dilute the focus of the paper. Including only works that are relevant to Computational Social Science and a quantitative discipline ensured the dataset’s relevance to computational methodologies. These filters may lead to an underrepresentation of emerging research with fewer citations. On the other hand, well-cited collaborative research within the traditional Computational Social Science disciplines may be overrepresented. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comsocsci2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
